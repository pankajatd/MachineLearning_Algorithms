{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jIg0MA9cFZaH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Sample initial dataset\n",
        "# ---------------------------\n",
        "df = pd.DataFrame({\n",
        "    'Feature1': [2, 5, 2, 2, 2],\n",
        "    'Feature2': [20, 50, 20, 20, 20],\n",
        "    'target': [1, 0, 1, 1, 1],\n",
        "})"
      ],
      "metadata": {
        "id": "a2hwou9AFazh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize equal weights\n",
        "df['weights'] = 1 / len(df)"
      ],
      "metadata": {
        "id": "RAsqdwY4Fa3g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Function to compute cumulative sums for weighted sampling\n",
        "# ---------------------------\n",
        "def compute_cumsum(df):\n",
        "    df['normalized_weights'] = df['weights'] / df['weights'].sum()\n",
        "    df['cumsum_upper'] = np.cumsum(df['normalized_weights'])\n",
        "    df['cumsum_lower'] = df['cumsum_upper'].shift(1, fill_value=0)\n",
        "    return df"
      ],
      "metadata": {
        "id": "on-F3bNnFbBo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Weighted resampling function\n",
        "# ---------------------------\n",
        "def create_new_dataset(df, n_samples=None):\n",
        "    if n_samples is None:\n",
        "        n_samples = len(df)\n",
        "    indices = []\n",
        "    for _ in range(n_samples):\n",
        "        a = np.random.random()\n",
        "        for index, row in df.iterrows():\n",
        "            if row['cumsum_lower'] <= a < row['cumsum_upper']:\n",
        "                indices.append(index)\n",
        "                break\n",
        "    return indices"
      ],
      "metadata": {
        "id": "FBP2YrEpFbEY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# AdaBoost-style resampling loop\n",
        "# ---------------------------\n",
        "max_iterations = 10\n",
        "iteration = 0\n",
        "datasets = []\n",
        "\n",
        "while iteration < max_iterations:\n",
        "    iteration += 1\n",
        "    print(f\"\\nIteration {iteration}:\")\n",
        "\n",
        "    # Step 1: Compute cumulative sums\n",
        "    df = compute_cumsum(df)\n",
        "\n",
        "    # Step 2: Sample new dataset according to weights\n",
        "    index_values = create_new_dataset(df)\n",
        "    new_df = df.loc[index_values, ['Feature1', 'Feature2', 'target', 'weights']].reset_index(drop=True)\n",
        "    datasets.append(new_df)\n",
        "\n",
        "    # Step 3: Train a weak learner (decision stump) using current weights\n",
        "    clf = DecisionTreeClassifier(max_depth=1)\n",
        "    clf.fit(new_df[['Feature1','Feature2']], new_df['target'], sample_weight=new_df['weights'])\n",
        "\n",
        "    # Step 4: Predict on the same dataset\n",
        "    new_df['y_pred'] = clf.predict(new_df[['Feature1','Feature2']])\n",
        "\n",
        "    # Step 5: Identify misclassified points\n",
        "    misclassified = new_df[new_df['y_pred'] != new_df['target']]\n",
        "    print(\"Number of misclassified points:\", len(misclassified))\n",
        "\n",
        "    # Stop if no misclassifications\n",
        "    if len(misclassified) == 0:\n",
        "        print(\"No misclassifications — stopping resampling.\")\n",
        "        break\n",
        "\n",
        "    # Step 6: Update weights for next iteration (boosting formula)\n",
        "    # Increase weights for misclassified points\n",
        "    alpha = 0.5  # learning rate for weight adjustment\n",
        "    df['weights'] = df['weights'] * np.exp(alpha * (df['target'] != new_df['y_pred']).astype(float))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xevO-OSdFbHo",
        "outputId": "c436a111-b27d-4baf-c1fb-0555024c6fbc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 1:\n",
            "Number of misclassified points: 0\n",
            "No misclassifications — stopping resampling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Final output\n",
        "# ---------------------------\n",
        "print(\"\\nFinal datasets:\")\n",
        "for i, d in enumerate(datasets):\n",
        "    print(f\"\\nDataset {i+1}:\")\n",
        "    print(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUSLPzdRFbKY",
        "outputId": "8b294152-c1de-4c79-acde-c65cc3cc61a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final datasets:\n",
            "\n",
            "Dataset 1:\n",
            "   Feature1  Feature2  target  weights  y_pred\n",
            "0         2        20       1      0.2       1\n",
            "1         2        20       1      0.2       1\n",
            "2         2        20       1      0.2       1\n",
            "3         2        20       1      0.2       1\n",
            "4         5        50       0      0.2       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WNHKrkR-FbNI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LRk1xg4aFbQI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4t4rb_JkFbTJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DVDpTQVoFbWM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-Z3Wy5JLFbZN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xrw5h1bcFbbd"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}